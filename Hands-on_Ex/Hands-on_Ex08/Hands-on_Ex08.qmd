---
title: "Hands-on Exercise 8: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method"
date: "March 5th, 2023"
date-modified: "`r Sys.Date()`"
number-sections: true
format: html
execute: 
  eval: true
  echo: true
  warning: false
editor: visual
---

# Importing Packages

```{r}
pacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary)
```

# Geospatial Data Wrangling

## Importing Data:

```{r}
mpsz = st_read(dsn = "data/geospatial", layer = "MP14_SUBZONE_WEB_PL")
```

## Updating CRS info

```{r}
mpsz_svy21 <- st_transform(mpsz, 3414)
```

```{r}
st_crs(mpsz_svy21)
```

```{r}
st_bbox(mpsz_svy21) #view extent
```

# Aspatial Data Wrangling

## Importing Aspatial Data:

```{r}
condo_resale = read_csv("data/aspatial/Condo_resale_2015.csv")
```

```{r}
glimpse(condo_resale)
```

```{r}
head(condo_resale$LONGITUDE) #see the data in XCOORD column
```

```{r}
head(condo_resale$LATITUDE) #see the data in YCOORD column
```

```{r}
summary(condo_resale)
```

## Converting aspatial dataframe into sf object

Converting condo_resale into a simple feature data frame and then converting the coordinates from wgs84 into svy21.

```{r}
condo_resale.sf <- st_as_sf(condo_resale,
                            coords = c("LONGITUDE", "LATITUDE"),
                            crs = 4326) %>%
  st_transform(crs = 3414)
```

```{r}
head(condo_resale.sf)
```

# EDA

## EDA using statistical graphics

Plotting the distribution of SELLING_PRICE.

```{r}
ggplot(data = condo_resale.sf, aes(x=`SELLING_PRICE`)) +
  geom_histogram(bins = 20, color="black", fill="light blue")
```

There is right skewed distribution, meaning that more condominium units were transacted at relative lower prices/

Statistically, the skewed dsitribution can be normalised by using log transformation. We can derive a new variable called LOG_SELLING_PRICE by doing a log transformation on the variable SELLING_PRICE. It is performed using mutate() of dplyr package.

```{r}
condo_resale.sf <- condo_resale.sf %>%
  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))
```

Now, we can plot LOG_SELLING_PRICE

```{r}
ggplot(data = condo_resale.sf, aes(x=`LOG_SELLING_PRICE`)) +
  geom_histogram(bins = 20, color="black", fill="light blue")
```

Now the distribution is relatively less skewed after transformation.

## Multiple Histogram Plots distribution of variables

In this section, we will draw small multiple histograms (trellis plot) using ggarrange() from ggpubr package. We will create 12 histograms, then use ggrrange() to organise the histograms into a 4 by 3 small multiple plot.

```{r}
AREA_SQM <- ggplot(data=condo_resale.sf, aes(x= `AREA_SQM`)) + 
  geom_histogram(bins=20, color="black", fill="light blue")

AGE <- ggplot(data=condo_resale.sf, aes(x= `AGE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_CBD <- ggplot(data=condo_resale.sf, aes(x= `PROX_CBD`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_CHILDCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_CHILDCARE`)) + 
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_ELDERLYCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_ELDERLYCARE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_URA_GROWTH_AREA <- ggplot(data=condo_resale.sf, 
                               aes(x= `PROX_URA_GROWTH_AREA`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_HAWKER_MARKET <- ggplot(data=condo_resale.sf, aes(x= `PROX_HAWKER_MARKET`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_KINDERGARTEN <- ggplot(data=condo_resale.sf, aes(x= `PROX_KINDERGARTEN`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_MRT <- ggplot(data=condo_resale.sf, aes(x= `PROX_MRT`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_PARK <- ggplot(data=condo_resale.sf, aes(x= `PROX_PARK`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_PRIMARY_SCH <- ggplot(data=condo_resale.sf, aes(x= `PROX_PRIMARY_SCH`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_TOP_PRIMARY_SCH <- ggplot(data=condo_resale.sf, 
                               aes(x= `PROX_TOP_PRIMARY_SCH`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

ggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, 
          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,
          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  
          ncol = 3, nrow = 4)
```

## Drawing Statistical Point Map

Revealing the geospatial distribution of condominium resale prices in SG.

```{r}
tmap_mode("view")
```

```{r}
# tm_shape(mpsz_svy21)+
#   tm_polygons() +
# tm_shape(condo_resale.sf) +
#   tm_dots(col = "SELLING_PRICE",
#           alpha = 0.6,
#           style="quantile") +
#   tm_view(set.zoom.limits = c(11,14))
```

```{r}
tmap_mode("plot")
```

# Hedonic Pricing Modelling

Now we will be building hedonic pricing models for condo resale units using lm().

## Simple Linear Regression Method

First, we will build a simple linear regression model by using SELLING_PRICE as the dependent variable and AREA_SQM as the independent variable.

```{r}
condo.slr <- lm(formula = SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)
```

lm() returns an object of class "lm" or for multiple responses of class c("mlm", "lm").

The functions summary() and anova() can be used to obtain and print a summary and analysis of variance table of the results. The generic accessor functions coefficients, effects, fitted.values and residuals extract various useful features of the value returned by lm.

```{r}
summary(condo.slr)
```

The output report reveals that the SELLING_PRICE can be explained by using the formula:

      *y = -258121.1 + 14719x1*

The R-squared of 0.4518 reveals that the simple regression model built is able to explain about 45% of the resale prices.

Since p-value is much smaller than 0.0001, we will reject the null hypothesis that mean is a good estimator of SELLING_PRICE. This will allow us to infer that simple linear regression model above is a good estimator of SELLING_PRICE.

The **Coefficients:** section of the report reveals that the p-values of both the estimates of the Intercept and AREA_SQM are smaller than 0.001. In view of this, the null hypothesis of the B0 and B1 are equal to 0 will be rejected. As a result, we will be able to infer that the B0 and B1 are good parameter estimates.

Visualising the best fit curve on a scatterplot by incormporating lm() as a method function in ggplot's geometry.

```{r}
ggplot(data=condo_resale.sf,  
       aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +
  geom_point() +
  geom_smooth(method = lm)
```

Figure reveals that there are a few statistical outliers with relatively high selling prices.

## Multiple Linear Regression Method

### Visualising the relationships of the independent variables

Before building a multiple regression model, we need to make sure the independent variables are not highly correlated to each other. If highly correlated variables are used to build a model, the quality will be compromised.

We will use corrplot() to plot a scatterplot matrix of the relationship between the independent variables in condo_resale.

```{r}
corrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = "AOE",
         tl.pos = "td", "tl.cex" = 0.5, method = "number", type = "upper")
```

Matrix reorder is important for mining the hidden strucutre and pattern in the matrix. There are 4 methods (AOE, FPC, hclust and alphabet). AOE orders the variables by using angular order of the eigenvectors method.

From the scatterplot matrix, it is clear that **Freehold** is highly correlated to **LEASE_99YEAR**. In view of this, it is wiser to only include either one of them in the subsequent model building. As a result, LEASE_99YEAR is excluded in the subsequent model building.

## Building a hedonic pricing model using multiple linear regression method

Calibrating the multiple linear regression model:

```{r}
condo.mlr <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE    + 
                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +
                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + 
                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + 
                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + 
                  PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                data=condo_resale.sf)
summary(condo.mlr)
```

## Preparing Publication Quality Table (olsrr method)

From the above report, it's clear that not all independent variables are statistically significant. We can revise the model by removing the variables that are not statistically significant.

Calibrating the revised model:

```{r}
condo.mlr1 <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + 
                   PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +
                   PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK + 
                   PROX_PRIMARY_SCH + PROX_SHOPPING_MALL    + PROX_BUS_STOP + 
                   NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,
                 data=condo_resale.sf)
ols_regress(condo.mlr1)
```

## Preparing Publication Quality Table (gtsummary method)

gtsummary package is an elegant and flexible way to create publication-ready summary tables. Using tlb_regression(), we can create a regression report.

```{r}
tbl_regression(condo.mlr1, intercept = TRUE)
```

We can append model statistics to the report table using add_glance_table() or add a table source note using add_glance_source_note().

```{r}
tbl_regression(condo.mlr1, 
               intercept = TRUE) %>% 
  add_glance_source_note(
    label = list(sigma ~ "\U03C3"),
    include = c(r.squared, adj.r.squared, 
                AIC, statistic,
                p.value, sigma))
```

### Test for multicolinearity

Here, we will be using ols_vif_tol() from olsrr to test for signs of multicollinearity.

```{r}
ols_vif_tol(condo.mlr1)
```

Since the VIF of the independent variables are less than 10, there are no signs of multicollinearity amongst the independent variables.

### Test for Non-Linearity

We have to test the assumption that linearity and additivity of the relationship between dependent and independent variables. We can use ols_plot_resid_fit() from olsrr to perform the linearity assumption test.

```{r}
ols_plot_resid_fit(condo.mlr1)
```

Most of the data points are scattered around the 0 line, hence the independent variables and dependent variables have a linear relationship.

### Test for Normality Assumption

Now we will test the assumption that the residuals follow a normal distribution.

```{r}
ols_plot_resid_hist(condo.mlr1)
```

The firgure shows the residuals of the MLR model follows a normal distribution.

Optionally, ols_test_normality() can be used for formal statistical test methods.

```{r}
ols_test_normality(condo.mlr)
```

The p-values for each of the 4 tests are much smaller than the alpha value of 0.05. Hence we will reject the null hypothesis and infer that there is statistical evidence that the residual are not normally distributed.

### Test for Spatial Autocorrelation

The hedonic model we are building is using geographically referenced attributes, hence it is also important for us to visuaisel the residual of the hedonic pricing model.

In order to perform spatial autocorrelation test, we need to convert condo_resale.sf from sf data frame into a SpatialPointsDataFrame.

First, we will export the residual of the hedonic pricing model and save it as a data frame.

```{r}
mlr.output <- as.data.frame(condo.mlr1$residuals)
```

Next we will join the newly created data frame with the condo_resale.sf object.

```{r}
condo_resale.res.sf <- cbind(condo_resale.sf, condo.mlr1$residuals) %>%
  rename(`MLR_RES` = `condo.mlr1.residuals`)
```

Next, we will convert the condo_resale.res.sf from simple feature object into SpatialPointsDataFrame because spdep package can only process sp conformed spatial data objects.

```{r}
condo_resale.sp <- as_Spatial(condo_resale.res.sf)
condo_resale.sp
```

Next, we will use tmpa to plot the distribution of the residuals on an interactive map.

```{r}
tmap_mode("view")

tm_shape(mpsz_svy21)+
  tmap_options(check.and.fix = TRUE) +
  tm_polygons(alpha = 0.4) +
tm_shape(condo_resale.res.sf) +  
  tm_dots(col = "MLR_RES",
          alpha = 0.6,
          style="quantile") +
  tm_view(set.zoom.limits = c(11,14))

tmap_mode("plot")
```

The diagram shows there is a sign of spatial autocorrelation. To prove the observation is true, we can perform the Moran's I test.

First, we will compute the distance-based weight matrix by using dnearneigh() from spdep.

```{r}
nb <- dnearneigh(coordinates(condo_resale.sp), 0, 1500, longlat = FALSE)
summary(nb)
```

Next, nb2listw() of spdep packge will be used to convert the output neighbours lists (i.e. nb) into a spatial weights.

```{r}
nb_lw <- nb2listw(nb, style = 'W')
summary(nb_lw)
```

Next, lm.morantest() of spdep package will be used to perform Moran's I test for residual spatial autocorrelation.

```{r}
lm.morantest(condo.mlr1, nb_lw)
```

The Global Moran's I test for residual spatial autocorrelation shows that it's p-value is less than 0.00000000000000022 which is less than the alpha value of 0.05. Hence, we will reject the null hypothesis that the residuals are randomly distributed.

Since the Observed Global Moran I = 0.1424418 which is greater than 0, we can infer than the residuals resemble cluster distribution.

# Building Hedonic Pricing Models using GWmodel

In this section, we will be building a hedonic pricing model using both **fixed** and **adaptive** bandwidth schemes.

## Computing Fixed Bandwidth

We will use bw.gwr() from gwmodel to determine the optimal fixed bandwidth to use for the model. The **adaptive** argument is set to FALSE because we are computing fixed bandwidth.

2 ways to determine the stopping rule: CV cross-validation approach and AIC corrected approach. The stopping rule is defined using the **approach** argument.

```{r}
bw.fixed <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + 
                     PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + 
                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + 
                     PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + 
                     FAMILY_FRIENDLY + FREEHOLD, 
                   data=condo_resale.sp, 
                   approach="CV", 
                   kernel="gaussian", 
                   adaptive=FALSE, 
                   longlat=FALSE)
```

The result shows that the recommended bandwidth is 971.3405 metres.

### GWModel method (Fixed Bandwidth)

Now we can calibrate the gwr model using fixed bandwidth and gaussian kernel.

```{r}
gwr.fixed <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + 
                         PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + 
                         PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + 
                         PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + 
                         FAMILY_FRIENDLY + FREEHOLD, 
                       data=condo_resale.sp, 
                       bw=bw.fixed, 
                       kernel = 'gaussian', 
                       longlat = FALSE)
```

```{r}
gwr.fixed
```

The report shows that the AICc of the gwr is 42263.61 which is significantly smaller than the globel multiple linear regression model of 42967.1.

## Computing Adaptive Bandwidth

Same as above, but we will set the adaptive argument to TRUE.

```{r}
bw.adaptive <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE  + 
                        PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE    + 
                        PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + 
                        PROX_PRIMARY_SCH + PROX_SHOPPING_MALL   + PROX_BUS_STOP + 
                        NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                      data=condo_resale.sp, 
                      approach="CV", 
                      kernel="gaussian", 
                      adaptive=TRUE, 
                      longlat=FALSE)
```

30 is the recommended data points to be used.

### GWModel method (Adaptive Bandwidth)

```{r}
gwr.adaptive <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + 
                            PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + 
                            PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + 
                            PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + 
                            NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                          data=condo_resale.sp, bw=bw.adaptive, 
                          kernel = 'gaussian', 
                          adaptive=TRUE, 
                          longlat = FALSE)

gwr.adaptive
```

The report shows that the AICc the adaptive distance gwr is 41982.22 which is even smaller than the AICc of the fixed distance gwr of 42263.61.

## Visualising GWR Output

In addition to regression residuals, the output feature class table includes fields for observed and predicted y values, condition number (cond), Local R2, residuals, and explanatory variable coefficients and standard errors:

-   Condition Number: this diagnostic evaluates local collinearity. In the presence of strong local collinearity, results become unstable. Results associated with condition numbers larger than 30, may be unreliable.

-   Local R2: these values range between 0.0 and 1.0 and indicate how well the local regression model fits observed y values. Very low values indicate the local model is performing poorly. Mapping the Local R2 values to see where GWR predicts well and where it predicts poorly may provide clues about important variables that may be missing from the regression model.

-   Predicted: these are the estimated (or fitted) y values 3. computed by GWR.

-   Residuals: to obtain the residual values, the fitted y values are subtracted from the observed y values. Standardized residuals have a mean of zero and a standard deviation of 1. A cold-to-hot rendered map of standardized residuals can be produce by using these values.

-   Coefficient Standard Error: these values measure the reliability of each coefficient estimate. Confidence in those estimates are higher when standard errors are small in relation to the actual coefficient values. Large standard errors may indicate problems with local collinearity.

They are all stored in a SpatialPointsDataFrame or SpatialPolygonsDataFrame object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its "data" slot in an object called SDF of the output list.

## Converting SDF into sf dataframe

To visualise the fields in SDF, we need to first covert it into sf data frame.

```{r}
condo_resale.sf.adaptive <- st_as_sf(gwr.adaptive$SDF) %>%
  st_transform(crs = 3414)
```

```{r}
condo_resale.sf.adaptive.svy21 <- st_transform(condo_resale.sf.adaptive, 3414)
```

```{r}
gwr.adaptive.output <- as.data.frame(gwr.adaptive$SDF)
condo_resale.sf.adaptive <- cbind(condo_resale.res.sf, as.matrix(gwr.adaptive.output))
```

```{r}
glimpse(condo_resale.sf.adaptive)
```

## Visualising local R2

```{r}
tmap_mode("view")
tm_shape(mpsz_svy21)+
  tm_polygons(alpha = 0.1) +
tm_shape(condo_resale.sf.adaptive) +  
  tm_dots(col = "Local_R2",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))
tmap_mode("plot")
```

## Visualising Coefficient Estimates

```{r}
tmap_mode("view")
AREA_SQM_SE <- tm_shape(mpsz_svy21)+
  tm_polygons(alpha = 0.1) +
tm_shape(condo_resale.sf.adaptive) +  
  tm_dots(col = "AREA_SQM_SE",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))

AREA_SQM_TV <- tm_shape(mpsz_svy21)+
  tm_polygons(alpha = 0.1) +
tm_shape(condo_resale.sf.adaptive) +  
  tm_dots(col = "AREA_SQM_TV",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))

tmap_arrange(AREA_SQM_SE, AREA_SQM_TV, 
             asp=1, ncol=2,
             sync = TRUE)
tmap_mode("plot")
```

### By URA Planning Region

```{r}
tm_shape(mpsz_svy21[mpsz_svy21$REGION_N=="CENTRAL REGION", ])+
  tm_polygons()+
tm_shape(condo_resale.sf.adaptive) + 
  tm_bubbles(col = "Local_R2",
           size = 0.15,
           border.col = "gray60",
           border.lwd = 1)
```


