---
title: "Take-home Exercise 3: Predicting HDB Public Housing Resale Pricies using Geographically Weighted Methods"
date: "9 March 2023"
date-modified: "`r Sys.Date()`"
number-sections: true
format: html
execute: 
  eval: true
  echo: true
  warning: false
editor: visual
---

# Overview

## Background

Housing is an essential component of household wealth worldwide. Buying a housing has always been a major investment for most people. The price of housing is affected by many factors. Some of them are global in nature such as the general economy of a country or inflation rate. Others can be more specific to the properties themselves. These factors can be further divided to **structural** and **locational** factors.

**Structural factors** are variables related to the property themselves such as the size, fitting, and tenure of the property. **Locational factors** are variables related to the neighbourhood of the properties such as proximity to childcare centre, public transport service and shopping centre.

## Problem

Conventional, housing resale prices predictive models were built by using **Ordinary Least Square (OLS)** method. However, this method failed to take into consideration that spatial autocorrelation and spatial heterogeneity exists in geographic data sets such as housing transactions.

With the existence of spatial autocorrelation, the OLS estimation of predictive housing resale pricing models could lead to biased, inconsistent, or inefficient results (Anselin 1998). In view of this limitation, **Geographical Weighted Models** were introduced for calibrating predictive models for housing resale prices.

## Task

In this take-home exercise, we will be creating models to **predict HDB resale prices at the sub-market level** (i.e. HDB 3-room, HDB 4-room and HDB 5-room) for the month of *January and February 2023* in Singapore.

We will be building the models using both the conventional OLS and GWR method. We will then compare the performance of the conventional OLS method versus the geographical weighted methods.

# Data

## Aspatial Data

-   [resale-flat-prices.csv](https://data.gov.sg/dataset/resale-flat-prices): HDB Resale Prices from 2017 onwards (.csv)

## Geospatial Data

| Name                                                                        | Format   | Source                                                                                                                                  |
|--------------------|--------------------|---------------------------------|
| MPSZ-2019                                                                   | .shp     | From Prof. Kam's In-Class Exercise Data                                                                                                 |
| MRT Locations                                                               | .shp     | [LTA Data Mall](https://datamall.lta.gov.sg/content/datamall/en/search_datasets.html?searchText=mrt)                                    |
| Bustop Locations                                                            | .shp     | [LTA Data Mall](https://datamall.lta.gov.sg/content/datamall/en/search_datasets.html?searchText=bus%20stop)                             |
| Eldercare Services                                                          | .shp     | [data.gov.sg](https://data.gov.sg/dataset/eldercare-services)                                                                           |
| Hawker Centers                                                              | .geojson | [data.gov.sg](https://data.gov.sg/dataset/hawker-centres)                                                                               |
| Parks                                                                       | .shp     | [data.gov.sg](https://dataportal.asia/dataset/192521342_parks/resource/0fe1f447-9964-47a8-b802-f97fa8fed6a0)                            |
| Supermarkets                                                                | .geojson | [data.gov.sg](https://dataportal.asia/dataset/192501037_supermarkets/resource/3933d0d7-a795-4c41-b611-e0e4f4697e54)                     |
| CHAS Clinics                                                                | .geojson | [data.gov.sg](https://dataportal.asia/dataset/192501037_chas-clinics)                                                                   |
| Childcare Centers                                                           | .geojson | [data.gov.sg](https://dataportal.asia/dataset/203030733_child-care-services/resource/d5d984bc-331b-4865-9c43-29588f7f78ff)              |
| Kindergarten/Preschool                                                      | .geojson | [data.gov.sg](https://dataportal.asia/dataset/203030733_pre-schools-location)                                                           |
| Shopping Malls                                                              | .csv     | [Mall Coordinates Web Scraper](https://github.com/ValaryLim/Mall-Coordinates-Web-Scraper)                                               |
| Primary Schools (extracted from the list on General information of schools) | .csv     | [data.gov.sg](https://dataportal.asia/dataset/192521611_school-directory-and-information/resource/ede26d32-01af-4228-b1ed-f05c45a1d8ee) |
| Good Primary Schools                                                        | \-       | [2022 Primary School Rankings](https://www.salary.sg/2022/best-primary-schools-2022-by-popularity/)                                     |

# Packages

The packages we will be using in this analysis are:

-   olsrr: R package for building OLS and performing diagnostics tests

-   ggpubr: creating and customising ggplot2 based publication ready plots

-   sf: importing, managing and processing geospatial data

-   spdep: creating spatial weight matrix objects, global and spatial autocorrelation statistics and related calculations

-   gwmodel: R package for calibrating geographical weighted family of models

-   tmap: choropleth mapping

-   tidyverse: attribute data handing

-   gtsummary: creating publication-ready summary tables

-   corrplot: Multivariate data visualisation and analysis

```{r}
pacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary, httr, jsonlite, SpatialML, devtools, rsample)
```

# Aspatial Data Wrangling

## Importing Aspatial Data

First, we will be using read_csv() to import resale-flat-prices.csv into R as a tibble data frame.

```{r}
#| eval: false
resale <- read_csv("data/aspatial/resale-flat-prices.csv")
```

Since the study requires us to focus on either three-room, four-room or five-room flat, I have decided to focus on 4 room flats. Hence, we need to filter the data to extract out the necessary data required.

# Filter Resale Data

filter() from dplyr is used to select the desired flat_type and stores it in resale dataframe.

```{r}
#| eval: false
resale <- resale %>%
  filter(flat_type == "4 ROOM")
```

## Transforming Resale Data

### Creating New Columns

We will be creating new columns using mutate():

-   address: concatenating block and street_name together using paste() function

-   remaining_lease_yr & remaining_lease_mth: splitting the year and months of the remaining_lease column using str_sub() and then converting the characters into integers using as.integer()

```{r}
#| eval: false
resale_transform <- resale %>%
  mutate(resale, address = paste(block,street_name)) %>%
  mutate(resale, remaining_lease_yr = as.integer(str_sub(remaining_lease, 0, 2))) %>%
  mutate(resale, remaining_lease_mth = as.integer(str_sub(remaining_lease, 9, 11)))
```

### Summing up remaining lease in months

Now we will replace NA values in the remaining_lease_mth column with 0 using is.na(). After which, we will multiply remaining_lease_yr by 12 to convert it into months. Then, we will create another column called "remaining_lease_mths" which sums the remaining_lease_year and remaining_lease_mth columns using rowSums().

```{r}
#| eval: false
resale_transform$remaining_lease_mth[is.na(resale_transform$remaining_lease_mth)] <- 0
resale_transform$remaining_lease_yr <- resale_transform$remaining_lease_yr * 12
resale_transform <- resale_transform %>% 
  mutate(resale_transform, remaining_lease_mths = rowSums(resale_transform[, c("remaining_lease_yr", "remaining_lease_mth")])) %>%
  select(month, town, address, block, street_name, flat_type, storey_range, floor_area_sqm, flat_model, 
         lease_commence_date, remaining_lease_mths, resale_price)
```

### Retrieving Postal Codes and Coordinates of Addresses

Now we will be retrieving postal codes and coordinates of the addresses so that we can get their proximity to locational factors.

**Creating a list storing unique addresses**

We create a list to store unique addresses to ensure that we do not run the GET request more than what is necessary We can also sort it to make it easier for us to see at which address the GET request will fail. Here, we use unique() function of base R package to extract the unique addresses then use sort() function of base R package to sort the unique vectors.

```{r}
#| eval: false
add_list <- sort(unique(resale_transform$address))
```

**Creating a function to retrieve coordinates**

Now we will create a function called get_coords to retrieve the coordinates from the OneMap.SG API.

1.  Create a dataframe called `postal_coords` to store all the final retrieved coordinates

2.  Use GET() function from httr package to make a GET request to the API

    -   OneMap SG offers functions for us to query spatial data from the API in a tidy format and provides additional functionalities to allow easy data manipulation.

    -   Here, we will be using their REST APIs to search address data for a given search value and retrieve the coordinates of the searched location.

    -   The required variables to be included in the GET request is as follows:

        -   **`searchVal`**: Keywords entered by user that is used to filter out the results.

        -   **`returnGeom`** {Y/N}: Checks if user wants to return the geometry.

        -   **`getAddrDetails`** {Y/N}: Checks if user wants to return address details for a point.

3.  Create a dataframe `new_row` which stores each final set of coordinates retrieved during the loop

4.  Check the number of responses returned and append to the main dataframe accordingly. We do this because:

    -   Num of returned responses of the searched location varies because some locations have only 1 result while others have multiple. Hence we need to first look at only those that do not have empty postal codes then take the first set of coordinates

    -   We can check to see if an address is invalid by looking at the number of rows returned by the request

5.  Lastly, we will append the returned response (`new_row`) with the necessary fields to the main dataframe (`postal_coords`) using *rbind()* function of base R package.

```{r}
#| eval: false
get_coords <- function(add_list){
  
  # Create a data frame to store all retrieved coordinates
  postal_coords <- data.frame()
    
  for (i in add_list){
    #print(i)

    r <- GET('https://developers.onemap.sg/commonapi/search?',
           query=list(searchVal=i,
                     returnGeom='Y',
                     getAddrDetails='Y'))
    data <- fromJSON(rawToChar(r$content))
    found <- data$found
    res <- data$results
    
    # Create a new data frame for each address
    new_row <- data.frame()
    
    # If single result, append 
    if (found == 1){
      postal <- res$POSTAL 
      lat <- res$LATITUDE
      lng <- res$LONGITUDE
      new_row <- data.frame(address= i, postal = postal, latitude = lat, longitude = lng)
    }
    
    # If multiple results, drop NIL and append top 1
    else if (found > 1){
      # Remove those with NIL as postal
      res_sub <- res[res$POSTAL != "NIL", ]
      
      # Set as NA first if no Postal
      if (nrow(res_sub) == 0) {
          new_row <- data.frame(address= i, postal = NA, latitude = NA, longitude = NA)
      }
      
      else{
        top1 <- head(res_sub, n = 1)
        postal <- top1$POSTAL 
        lat <- top1$LATITUDE
        lng <- top1$LONGITUDE
        new_row <- data.frame(address= i, postal = postal, latitude = lat, longitude = lng)
      }
    }

    else {
      new_row <- data.frame(address= i, postal = NA, latitude = NA, longitude = NA)
    }
    
    # Add the row
    postal_coords <- rbind(postal_coords, new_row)
  }
  return(postal_coords)
}
```

**Calling the function to retrieve coordinates**

```{r}
#| eval: false
coords <- get_coords(add_list)
```

**Inspecting results**

Here, we check whether the relevant columns contains any NA values with is.na() function of base R package and also "NIL".

```{r}
#| eval: false
coords[(is.na(coords$postal) | is.na(coords$latitude) | is.na(coords$longitude) | coords$postal=="NIL"), ]
```

From this, we notice that the address "215 CHOA CHU KANG CTRL" and "216 CHOA CHU KANG CTRL" do not have a postal code but has geographic coordinates. Upon further research, it has the postal code 608215 and 680216 respectively. However, as the API returned the same set of coordinates for both of these addresses, we shall proceed with keeping them as we are more interested in the coordinates for our analysis later on.

**Combining resale and coordinates data**

After retrieving the coordinates, we should combine the successful ones with our transformed resale dataset. We can do this by using left_join() function of dplyr package.

```{r}
#| eval: false
rs_coords <- left_join(resale_transform, coords, by = c('address' = 'address'))
```

## Creating rds file

Now that the resale dataset is now complete with the coordinates, we can save it in an rds file. This also prevents is from needing to run the GET request everytime.

```{r}
#| eval: false
rs_coords_rds <- write_rds(rs_coords, "data/rds/rs_coords.rds")
```

## Importing RDS file

```{r}
#| eval: false
rs_coords <- read_rds("data/rds/rs_coords.rds")
glimpse(rs_coords)
```

### Transform and Checking CRS

Since the coordinates (latitude and longitude) are in decimal degrees, the projected CRS is WGS84. We will need to assign them WGS84's EPSG code 4326 first before transforming it to 3414, the EPSG code for SVY21. st_as_sf() converts the dataframe into an sf object, then st_transform() transforms the coordinates into the appropriate CRS.

```{r}
#| eval: false
rs_coords_sf <- st_as_sf(rs_coords,
                    coords = c("longitude", 
                               "latitude"),
                    crs=4326) %>%
  st_transform(crs = 3414)

st_crs(rs_coords_sf)
```

# Locational Factors Data Wrangling

## Locational Factors with geographic coordinates

### Importing Data & Checking CRS

::: panel-tabset
#### Importing Data

```{r}
#| eval: false
eldercare <- st_read(dsn = "data/locational", layer = "ELDERCARE")
```

```{r}
#| eval: false
mrt <- st_read(dsn = "data/locational", layer = "Train_Station_Exit_Layer")
```

```{r}
#| eval: false
bus <- st_read(dsn = "data/locational", layer = "BusStop")
```

```{r}
#| eval: false
parks <- st_read(dsn = "data/locational", layer = "NATIONALPARKS")
```

```{r}
#| eval: false
hawker <- st_read("data/locational/hawker-centres-geojson.geojson")
```

```{r}
#| eval: false
supermarket <- st_read("data/locational/supermarkets.geojson")
```

```{r}
#| eval: false
clinic <- st_read("data/locational/moh-chas-clinics.geojson")
```

```{r}
#| eval: false
childcare <- st_read("data/locational/childcare.geojson")
```

```{r}
#| eval: false
kindergarten <- st_read("data/locational/preschools-location.geojson")
```

```{r}
#| eval: false
malls <- read_csv("data/locational/mall_coordinates_updated.csv")
mall_sf <- st_as_sf(malls, coords = c("longitude", "latitude"), crs=4326)
```

#### Check CRS

```{r}
#| eval: false
st_crs(eldercare)
```

```{r}
#| eval: false
st_crs(mrt)
```

```{r}
#| eval: false
st_crs(bus)
```

```{r}
#| eval: false
st_crs(parks)
```

```{r}
#| eval: false
st_crs(hawker)
```

```{r}
#| eval: false
st_crs(supermarket)
```

```{r}
#| eval: false
st_crs(clinic)
```

```{r}
#| eval: false
st_crs(childcare)
```

```{r}
#| eval: false
st_crs(kindergarten)
```

```{r}
#| eval: false
st_crs(mall_sf)
```
:::

From the Results:

-   Datasets with WGS48 Geodetic CRS: hawker, supermarket, clinic, childcare, kindergarten, malls_sf. We will need to transform the CRS into SVY21

-   Datasets with SVY21 as Projected CRS: eldercare, mrt, bus, parks. For all of them, the EPSG code is 9001, which is wrong. We need to change the CRS into the correct EPSG code for SVY21, **3414**

### Assigning EPSG code and Transforming CRS

::: panel-tabset
#### Code

```{r}
#| eval: false
eldercare <- st_set_crs(eldercare, 3414)
mrt <- st_set_crs(mrt, 3414)
bus <- st_set_crs(bus, 3414)
parks <- st_set_crs(parks, 3414)

hawker <- hawker %>%
  st_transform(crs=3414)
supermarket <- supermarket %>%
  st_transform(crs=3414)
clinic <- clinic %>%
  st_transform(crs=3414)
childcare <- childcare %>%
  st_transform(crs=3414)
kindergarten <- kindergarten %>%
  st_transform(crs=3414)
mall_sf <- st_transform(mall_sf, crs=3414)
```

#### Check CRS

```{r}
#| eval: false
st_crs(eldercare)
```

```{r}
#| eval: false
st_crs(mrt)
```

```{r}
#| eval: false
st_crs(bus)
```

```{r}
#| eval: false
st_crs(parks)
```

```{r}
#| eval: false
st_crs(hawker)
```

```{r}
#| eval: false
st_crs(supermarket)
```

```{r}
#| eval: false
st_crs(clinic)
```

```{r}
#| eval: false
st_crs(childcare)
```

```{r}
#| eval: false
st_crs(kindergarten)
```

```{r}
#| eval: false
st_crs(mall_sf)
```
:::

From the above results, we can see that the EPSG code of all the data has now been assigned correctly and they are all EPSG 3414.

### Calculating Proximity

#### Creating get_prox function

```{r}
#| eval: false
get_prox <- function(origin_df, dest_df, col_name){
  
  # creates a matrix of distances
  dist_matrix <- st_distance(origin_df, dest_df)           
  
  # find the nearest location_factor and create new data frame
  near <- origin_df %>% 
    mutate(PROX = apply(dist_matrix, 1, function(x) min(x)) / 1000) 
  
  # rename column name according to input parameter
  names(near)[names(near) == 'PROX'] <- col_name

  # Return df
  return(near)
}
```

What the code does:

-   create a matrix of distances between the HDB and the locational factor using st_distance of sf package.

-   get the nearest point of the locational factor by looking at the minimum distance using min function of base R package then add it to HDB resale data under a new column using mutate() function of dpylr package.

-   rename the column name according to input given by user so that the columns have appropriate and distinct names that are different from one another.

#### Calling get_prox function

We will be calling the function to get the proximity of the resale HDB flats and the locational factors such as: Eldercare, MRT, Hawker, Parks, Supermarkets, Clinics

Then we will create a new column in rs_coords_sf dataframe to store the proximity

```{r}
#| eval: false
rs_coords_sf <- get_prox(rs_coords_sf, eldercare, "PROX_ELDERLYCARE") 
rs_coords_sf <- get_prox(rs_coords_sf, mrt, "PROX_MRT") 
rs_coords_sf <- get_prox(rs_coords_sf, hawker, "PROX_HAWKER") 
rs_coords_sf <- get_prox(rs_coords_sf, parks, "PROX_PARK") 
rs_coords_sf <- get_prox(rs_coords_sf, supermarket, "PROX_SUPERMARKET")
rs_coords_sf <- get_prox(rs_coords_sf, clinic, "PROX_CLINIC")
rs_coords_sf <- get_prox(rs_coords_sf, mall_sf, "PROX_MALL")
```

### Creating get_within function

Now we will create a get_within function to calculate the number of factors within a specific distance.

```{r}
#| eval: false
get_within <- function(origin_df, dest_df, threshold_dist, col_name){
  
  # creates a matrix of distances
  dist_matrix <- st_distance(origin_df, dest_df)   
  
  # count the number of location_factors within threshold_dist and create new data frame
  wdist <- origin_df %>% 
    mutate(WITHIN_DT = apply(dist_matrix, 1, function(x) sum(x <= threshold_dist)))
  
  # rename column name according to input parameter
  names(wdist)[names(wdist) == 'WITHIN_DT'] <- col_name

  # Return df
  return(wdist)
}
```

What the function does:

-   create a matrix of distances between the HDB and the locational factor using st_distance of sf package.

-   get the sum of points of the locational factor that are within the threshold distance using sum function of base R package then add it to HDB resale data under a new column using mutate() function of dpylr package.

-   rename the column name according to input given by user so that the columns have appropriate and distinct names that are different from one another.

#### Calling get_within function

Here, we call the get_within function created earlier to get the number of locational factors that are within a certain threshold distance.

The threshold we set it to will be Within 350m for locational factors such as, Kindergartens, Childcare centres and Bus stops.

```{r}
#| eval: false
rs_coords_sf <- get_within(rs_coords_sf, kindergarten, 350, "WITHIN_350M_KINDERGARTEN")
rs_coords_sf <- get_within(rs_coords_sf, childcare, 350, "WITHIN_350M_CHILDCARE")
rs_coords_sf <- get_within(rs_coords_sf, bus, 350, "WITHIN_350M_BUS")
```

## Locational Factors without geographic coordinates

In this section, we retrieve those locational factors that are not easily obtainable from data.gov.sg and/or does not have any geographic coordinates.

### CBD

We are required to get the proximity of the resale HDBs to the CBD. Based on a quick search, the latitude and longitude of Downtown Core (also known as CBD) are **1.287953** and **103.851784** respectively.

Since we already have the geographic coordinates of the resale data, we just need to convert the latitude and longitude of CBD into EPSG 3414 (SVY21) before running the get_prox function.

We will first create a dataframe consisting of the latitude and longitude coordinates of the CBD area then transform it to EPSG 3414 (SVY21) format.

**Storing CDB coordinates in a dataframe:**

```{r}
#| eval: false
name <- c('CBD Area')
latitude= c(1.287953)
longitude= c(103.851784)
cbd_coords <- data.frame(name, latitude, longitude)
```

**Assigning and transforming CRS**

```{r}
#| eval: false
cbd_coords_sf <- st_as_sf(cbd_coords,
                    coords = c("longitude", 
                               "latitude"),
                    crs=4326) %>%
  st_transform(crs = 3414)

st_crs(cbd_coords_sf)
```

**Calling get_prox function**

```{r}
#| eval: false
rs_coords_sf <- get_prox(rs_coords_sf, cbd_coords_sf, "PROX_CBD")
```

### Primary Schools

**Reading CSV File**

```{r}
#| eval: false
pri_sch <- read_csv("data/locational/general-information-of-schools.csv")
```

**Extracting Primary Schools and Required Columns**

```{r}
#| eval: false
pri_sch <- pri_sch %>%
  filter(mainlevel_code == "PRIMARY") %>%
  select(school_name, address, postal_code, mainlevel_code)
```

**Creating a list to store postal codes**

```{r}
#| eval: false
prisch_list <- sort(unique(pri_sch$postal_code))
```

**Calling get_coords function to get coordinates of Primary Schools**

```{r}
#| eval: false
prisch_coords <- get_coords(prisch_list)
```

```{r}
#| eval: false
prisch_coords[(is.na(prisch_coords$postal) | is.na(prisch_coords$latitude) | is.na(prisch_coords$longitude)),]
```

The postal codes of the schools with NA values in the postal code, latitude and longitude are:

-   319133 (Pei Chun Public School)
-   228091 (St. Margaret's Primary School)

This is due to the postal codes and the addresses in our data not matching with the postal codes and addresses in the API. Let's quickly modify those:

```{r}
#| eval: false
#Pei Chun Public School
pri_sch[pri_sch$school_name == "PEI CHUN PUBLIC SCHOOL", "address"] <- "16 LORONG 7 TOA PAYOH"
pri_sch[pri_sch$school_name == "PEI CHUN PUBLIC SCHOOL", "postal_code"] <- "319320"

#St. Margaret's Primary School
pri_sch[pri_sch$school_name == "ST. MARGARET'S PRIMARY SCHOOL", "address"] <- "2 MATTAR ROAD"
pri_sch[pri_sch$school_name == "ST. MARGARET'S PRIMARY SCHOOL", "postal_code"] <- "387724"
```

Updating the coordinates dataframe again:

```{r}
#| eval: false
prisch_list <- sort(unique(pri_sch$postal_code))
```

```{r}
#| eval: false
prisch_coords <- get_coords(prisch_list)
```

Checking again to make sure we have no more NA values:

```{r}
#| eval: false
prisch_coords[(is.na(prisch_coords$postal) | is.na(prisch_coords$latitude) | is.na(prisch_coords$longitude)),]
```

0 rows with NA values.

**Combining coordinates with Primary School Names**

Here, we combine the retrieved coordinates with the df that has the Primary School Names so that we can verify whether we have extracted it correctly. We combine it using the left_join function of dplyr package.

```{r}
#| eval: false
prisch_coords = prisch_coords[c("postal","latitude", "longitude")]
pri_sch <- left_join(pri_sch, prisch_coords, by = c('postal_code' = 'postal'))
```

Checking again for any NA values:

```{r}
#| eval: false
pri_sch[(is.na(pri_sch$latitude) | is.na(pri_sch$longitude)),]
```

We have successfully retrived all the coordinates of the primary schools.

**Converting pri_sch dataframe into an sf object and transforming CRS**

We will use st_as_sf() function of sf package to convert the data frame into sf object, and then use st_transform() function of sf package to transform the coordinates of the sf object.

```{r}
#| eval: false
prisch_sf <- st_as_sf(pri_sch,
                    coords = c("longitude", 
                               "latitude"),
                    crs=4326) %>%
  st_transform(crs = 3414)

st_crs(prisch_sf)
```

**Calling get_within function**

Now we will call the get_within function to get the number of primary schools within the 1km threshold.

```{r}
#| eval: false
rs_coords_sf <- get_within(rs_coords_sf, prisch_sf, 1000, "WITHIN_1KM_PRISCH")
```

### Good Primary Schools

We need to extract the list of "good" primary schools from online websites since there are no datasets available. In particular, we can look at salary.sg for the list of Best Primary Schools of 2022. Here is the top 10 list:

![](images/image-159004597.png)

```{r}
#| eval: false
#creating a dataframe to store the schools
school_name <- c("METHODIST GIRLS' SCHOOL (PRIMARY)",
                 "CATHOLIC HIGH SCHOOL",
                 "TAO NAN SCHOOL",
                 "PEI HWA PRESBYTERIAN PRIMARY SCHOOL",
                 "HOLY INNOCENTS' PRIMARY SCHOOL",
                 "NAN HUA PRIMARY SCHOOL",
                 "CHIJ SAINT. NICHOLAS GIRLS' SCHOOL",
                 "ADMIRALTY PRIMARY SCHOOL",
                 "SAINT. HILDA'S PRIMARY SCHOOL",
                 "AI TONG SCHOOL")
top_good_pri <- data.frame(school_name)
```

**Getting coordinates of the schools**

We can call out get_coords function to retrieve the coordinates of the top 10 primary schools. But first, we need to store the school names in a list.

```{r}
#| eval: false
good_pri_list <- unique(top_good_pri$school_name)
```

```{r}
#| eval: false
goodprisch_coords <- get_coords(good_pri_list)
```

**Inspecting results**

```{r}
#| eval: false
goodprisch_coords[(is.na(goodprisch_coords$postal) | is.na(goodprisch_coords$latitude) | is.na(goodprisch_coords$longitude)), ]
```

No values with NA, all the coordinates of the good primary schools have been retrived successfully.

**Converting goodprisch_coords dataframe into an sf object and transforming CRS**

```{r}
#| eval: false
goodpri_sf <- st_as_sf(goodprisch_coords,
                    coords = c("longitude", 
                               "latitude"),
                    crs=4326) %>%
  st_transform(crs = 3414)
st_crs(goodpri_sf)
```

**Getting proximity to good primary schools**

Now we call the get_prox function to get the proximity of the HDB flats and good primary schools.

```{r}
#| eval: false
rs_coords_sf <- get_prox(rs_coords_sf, goodpri_sf, "PROX_GOOD_PRISCH")
```

## Writing to RDS file

Now our resale data is complete with all the locational factors. We can save it in an rds file so that we do not have to keep running all the codes above.

```{r}
#| eval: false
rs_factors_rds <- write_rds(rs_coords_sf, "data/rds/rs_factors.rds")
```

# Encoding Locational Factors

## Reading RDS File

```{r}
#| eval: false
rs_sf <- read_rds("data/rds/rs_factors.rds")
glimpse(rs_sf)
```

From the dataframe, we can see that storey_range is in character type. It is a categorical variable, which cannot be used in the regression equation just as it is. It needs to be recoded into a series of variables which then can be used in the regression model.

We can use "dummy coding" where it consists of creating dichotomous variables where each level of the categorical variable is contrasted to a specified reference level.

However, some categorical variables have levels that are ordered. Hence, they can be converted to numerical values instead and used as is. In this scenario, storey_range can be ranked from low to high. By doing so, we can gain insight as to whether higher/lower floors affect the resale price.

Hence, instead of using dummy variables, we will be using sorting the storey_range categorical variable and assigning numerical values that are in ascending order.

## Extracting unique storey_range and sort

```{r}
#| eval: false
storeys <- sort(unique(rs_sf$storey_range))
```

## Creating dataframe storey_range_order to store the order of storey_range

```{r}
#| eval: false
storey_order <- 1:length(storeys)
storey_range_order <- data.frame(storeys, storey_order)

head(storey_range_order)
```

## Combining storey_order with resale dataframe

```{r}
#| eval: false
rs_sf <- left_join(rs_sf, storey_range_order, by= c("storey_range" = "storeys"))

glimpse(rs_sf)
```

## Selecting required columns for analysis

```{r}
#| eval: false
rs_req <- rs_sf %>%
  select(month, resale_price, floor_area_sqm, storey_order, remaining_lease_mths,
         PROX_CBD, PROX_ELDERLYCARE, PROX_HAWKER, PROX_MRT, PROX_PARK, PROX_GOOD_PRISCH, PROX_MALL, PROX_CLINIC,
         PROX_SUPERMARKET, WITHIN_350M_KINDERGARTEN, WITHIN_350M_CHILDCARE, WITHIN_350M_BUS, WITHIN_1KM_PRISCH)
```

```{r}
#| eval: false
glimpse(rs_req)
```

## Writing to rds file

This is the final resale data file we will use for our model later.

```{r}
#| eval: false
resale_final <- write_rds(rs_req, "data/rds/resale_final.rds")
```

## Viewing Summary

```{r}
resale_final <- read_rds("data/rds/resale_final.rds")
summary(resale_final)
```

# Geospatial Data Wrangling

## Importing Geospatial Data

```{r}
mpsz <- st_read(dsn = "data/geospatial", layer = "MPSZ-2019")
```

Obtaining coordinate system using st_crs()

```{r}
st_crs(mpsz)
```

The projected CRS for mpsz dataframe is WGS84, but we need it to be in SVY21 with EPSG code 3414.

```{r}
mpsz <- st_transform(mpsz, 3414)
st_crs(mpsz)
```

Its is now in the correct EPSG code.

Checking for invalid geometry:

```{r}
length(which(st_is_valid(mpsz) == FALSE))
```

```{r}
#making the geometries valid
mpsz <- st_make_valid(mpsz)
length(which(st_is_valid(mpsz) == FALSE))
```

```{r}
st_bbox(mpsz)
```

# Exploratory Data Analysis

## Statistical Graphs

We can plot the distribution of resale_price.

```{r}
ggplot(data=resale_final, aes(x=`resale_price`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
```

There is a right skewed distribution, meaning that more of the 4-Room flats were transacted at relative lower prices. Statistically, the skewed distribution can be normalised by doing log transformation. We can derive a new variable called log_resale_price by doing a log transformation on the variable resale_price by using mutate() of dplyr package.

```{r}
resale_final <- resale_final %>%
  mutate(`log_resale_price` = log(resale_price))
```

Plotting the distribution of log_resale_price:

```{r}
ggplot(data=resale_final, aes(x=`log_resale_price`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
```
We can see that the post-transformation distribution is relatively less skewed, but we will still use the original resale_price variable as log_resale_price has high correlation with actual resale price.


# Predictive Modelling

Now we will be creating a predictive model to predict the HDB resale prices. We will be using the Ordinary Least Squares method and Geographical Weighted Random Forest Method.




